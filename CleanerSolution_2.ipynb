{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from hashlib import sha1\n",
    "from collections import defaultdict\n",
    "from model_saver import save_params_scores\n",
    "plt.rcParams['figure.figsize'] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_str(ss):\n",
    "    return ss.replace(' Go', 'Go').lower().replace(': ', ':').replace('+', ' ').replace('; ', ';').replace(' ;', ';'\n",
    "           ).replace('=5000', ' 5000f').replace(' =', '=').replace('= ', '=').replace('on-net', ' ONNET ').replace('onnet', ' ONNET '\n",
    "           ).replace('on net', ' ONNET ').replace('all-net', ' ALLNET ').replace('allnet', ' ALLNET '\n",
    "           ).replace('_', ' ').replace('1 month', '30d').replace('24', ' 24').replace(' f=', 'f='\n",
    "           ).replace('0 f', '0f').replace('0=', '0f=').replace('zone ', 'ZONE').replace('0o', '0f o'\n",
    "           ).replace('cvm', ' CVM ').replace('apanews', ' APANEWS ').replace('evc', ' EVC '\n",
    "           ).replace('fifa ts', ' FIFATS ').replace('jokko', ' JOKKO ').replace('ivr', ' IVR '\n",
    "           ).replace('mixt', ' MIXT ').replace('youth', 'youth ').replace('youth', ' YOUTH '\n",
    "           ).replace('new clir', ' NEWCLIR ').replace('pilot', ' PILOT ').replace('wifi family', ' WIFIFAMILY '\n",
    "           ).replace('fnf', 'FNF').replace('supermagik ', 'SUPERMAGIK').replace('0 o', '0f o'\n",
    "           ).replace('unlimited', 'UNLIMITED').replace('30 days', '30d').replace('0 mo', '0mo'\n",
    "           ).replace(' mb', 'mb').replace('=', ' ').replace(';', ' ').replace(',', ' ').replace(':', ' '\n",
    "           ).replace('(', ' ').replace(')', ' ').replace('0f', '0f ').replace('off net', 'offnet').replace('mega', 'mega '\n",
    "           ).replace('12500', '12500f').replace('off', 'offnet').replace('on', 'ONNET').replace('20k', '20000f '\n",
    "           ).replace('80k', '80000f ').replace('offnetnet', 'offnet').replace('offnet', ' offnet ').replace('500 ', '500f '\n",
    "           ).replace('600 ', '600f ').replace('5000 ', '5000f ').replace('mONNETth', 'month ').replace('1250  O', '1250f O'\n",
    "           ).replace('500f 5000', '500f 5000f').replace('zONNETe', 'zone').replace('0ff', '0f').replace('1250 ONNET', '1250f ONNET'\n",
    "           ).replace('00h-08h', '8h').replace('offnet er', 'offer').replace('4000 ', '4000f ').replace('O 30', 'O 30d'\n",
    "           ).replace('24h/', '24h ').replace('equal', 'f = ').replace('ALLNET 1000 ', 'ALLNET 1000 ').replace('  ', ' '\n",
    "           ).replace('  ', ' ').replace('  ', ' ').strip().lower().replace(' offnet', 'offnet').replace(' onnet', 'onnet').replace(' allnet', 'allnet'\n",
    "           ).replace('fonnete', 'fonnet')\n",
    "\n",
    "def embd_string(ss):\n",
    "    string_processed = proc_str(ss)\n",
    "    num_groups = re.findall(r'([\\d\\.]+)([^\\d\\s]+)', string_processed)\n",
    "    \n",
    "    num_groups_dict = {}\n",
    "    for tp in ['milf', 'f', 'f2', 'fonnet', 'fallnet', 'foffnet', 'mnonnet', 'mnoffnet', 'mnallnet', 'honnet', 'hoffnet', 'hallnet', 'mo', 'go', 'sms', 'd', 'gportal', 'day', 'mb', 'gb', 'mbps', 'h', 'opia', 'mn']:\n",
    "        num_groups_dict[tp] = []\n",
    "    if num_groups:\n",
    "        for num, tp in num_groups:\n",
    "            num_groups_dict[tp].append(float(num))\n",
    "    for tp in ['milf', 'f', 'f2', 'fonnet', 'fallnet', 'foffnet', 'mnonnet', 'mnoffnet', 'mnallnet', 'honnet', 'hoffnet', 'hallnet', 'mo', 'go', 'sms', 'd', 'gportal', 'day', 'mb', 'gb', 'mbps', 'h', 'opia', 'mn']:\n",
    "        if not num_groups_dict[tp]:\n",
    "            num_groups_dict[tp].append(0)\n",
    "        elif len(num_groups_dict[tp]) > 1:\n",
    "            num_groups_dict['f2'] = [num_groups_dict[tp][1]]\n",
    "            num_groups_dict[tp] = [num_groups_dict[tp][0]]\n",
    "            \n",
    "    return np.concatenate([cvec.transform([re.sub(r'([\\d\\.]+)([^\\d\\s]+)', '', ww)]).toarray()[0], np.array(list(num_groups_dict.values())).flatten()], dtype=np.float32)\n",
    "\n",
    "\n",
    "def create_mapping(dftr, dfts):\n",
    "    cvec = CountVectorizer(token_pattern='\\S+')\n",
    "    cvec.fit([re.sub(r'([\\d\\.]+)([^\\d\\s]+)', '', wtp) for wtp in to_process])\n",
    "    to_process = []\n",
    "    mapping_toppack = {}\n",
    "\n",
    "    for ss in sorted(list(set(\n",
    "        dftr[~dftr['TOP_PACK'].isna()]['TOP_PACK'].unique()\n",
    "    ).union(set(\n",
    "        dfts[~dfts['TOP_PACK'].isna()]['TOP_PACK'].unique()\n",
    "    )))):\n",
    "        to_process.append(proc_str(ss))\n",
    "\n",
    "    for ss in sorted(list(set(\n",
    "        dftr[~dftr['TOP_PACK'].isna()]['TOP_PACK'].unique()\n",
    "    ).union(set(\n",
    "        dfts[~dfts['TOP_PACK'].isna()]['TOP_PACK'].unique()\n",
    "    )))):\n",
    "        mapping_toppack[ss] = embd_string(ss)\n",
    "\n",
    "    mapping_toppack[dfts[dfts['TOP_PACK'].isna()].loc[:, 'TOP_PACK'].values[0]] = np.zeros_like(embd_string(ss), dtype=np.float32)\n",
    "    return mapping_toppack, len(embd_string(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gb_features(dftr, dfts, categorical_column, continious_column_can_be_filled):\n",
    "    feature_name_diff = f'GB_DIFF_FEATURE__{categorical_column}__{continious_column_can_be_filled}'\n",
    "    feature_name = f'GB_FEATURE__{categorical_column}__{continious_column_can_be_filled}'\n",
    "    gb = dftr.groupby([categorical_column])[continious_column_can_be_filled].mean()\n",
    "    msk_dftr = ~dftr[categorical_column].isna()\n",
    "    msk_dfts = ~dfts[categorical_column].isna()\n",
    "    dftr.loc[msk_dftr, feature_name_diff] = dftr[msk_dftr][[categorical_column, continious_column_can_be_filled + '__FILLED']].swifter.apply(\n",
    "        lambda x: x[continious_column_can_be_filled + '__FILLED'] - gb[x[categorical_column]], axis=1\n",
    "    )\n",
    "    dfts.loc[msk_dfts, feature_name_diff] = dfts[msk_dfts][[categorical_column, continious_column_can_be_filled + '__FILLED']].swifter.apply(\n",
    "        lambda x: x[continious_column_can_be_filled + '__FILLED'] - gb[x[categorical_column]], axis=1\n",
    "    )\n",
    "    na_mean = dftr[~msk_dftr][continious_column_can_be_filled].mean()\n",
    "    dftr.loc[~msk_dftr, feature_name_diff] = dftr[continious_column_can_be_filled + '__FILLED'] - na_mean\n",
    "    dfts.loc[~msk_dfts, feature_name_diff] = dfts[continious_column_can_be_filled + '__FILLED'] - na_mean\n",
    "    \n",
    "    dftr[feature_name] = dftr[feature_name_diff] + dftr[continious_column_can_be_filled + '__FILLED']\n",
    "    dfts[feature_name] = dfts[feature_name_diff] + dfts[continious_column_can_be_filled + '__FILLED']\n",
    "    \n",
    "    return dftr, dfts, [feature_name_diff, feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_target_encoding(dftr, dfts, feature_name):\n",
    "    dd = dict(dftr.groupby(feature_name)['CHURN'].mean())\n",
    "    ddd = dict(zip(np.sort(dftr.groupby(feature_name)['CHURN'].mean()), np.arange(len(dd))))\n",
    "    mapping = {k: ddd[v] for k, v in dd.items()}\n",
    "    dftr[feature_name + '__RANK_ENCODED'] = dftr[feature_name].map(mapping)\n",
    "    dfts[feature_name + '__RANK_ENCODED'] = dfts[feature_name].map(mapping)\n",
    "    return dftr, dfts, feature_name + '__RANK_ENCODED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(dftr, dfts):\n",
    "    new_features = []\n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('NANS_NUM')\n",
    "    dftr['NANS_NUM'] = dftr.isna().sum(1)\n",
    "    dfts['NANS_NUM'] = dfts.isna().sum(1)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('REVENUE__FILLED')\n",
    "    dftr['REVENUE__FILLED'] = dftr['REVENUE'].fillna(0)\n",
    "    dfts['REVENUE__FILLED'] = dfts['REVENUE'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('FREQUENCE__FILLED')\n",
    "    dftr['FREQUENCE__FILLED'] = dftr['FREQUENCE'].fillna(0)\n",
    "    dfts['FREQUENCE__FILLED'] = dfts['FREQUENCE'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('FREQUENCE_RECH__FILLED')\n",
    "    dftr['FREQUENCE_RECH__FILLED'] = dftr['FREQUENCE_RECH'].fillna(0)\n",
    "    dfts['FREQUENCE_RECH__FILLED'] = dfts['FREQUENCE_RECH'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('REVENUE__*__FREQUENCE_RECH')\n",
    "    dftr['REVENUE__*__FREQUENCE_RECH'] = dftr['REVENUE__FILLED'] * dftr['FREQUENCE_RECH__FILLED']\n",
    "    dfts['REVENUE__*__FREQUENCE_RECH'] = dfts['REVENUE__FILLED'] * dfts['FREQUENCE_RECH__FILLED']\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('REVENUE__*__FREQUENCE')\n",
    "    dftr['REVENUE__*__FREQUENCE'] = dftr['REVENUE__FILLED'] * dftr['FREQUENCE__FILLED']\n",
    "    dfts['REVENUE__*__FREQUENCE'] = dfts['REVENUE__FILLED'] * dfts['FREQUENCE__FILLED']\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('REVENUE__*__FREQUENCE__-__REVENUE__*__FREQUENCE_RECH')\n",
    "    dftr['REVENUE__*__FREQUENCE__-__REVENUE__*__FREQUENCE_RECH'] = dftr['REVENUE__*__FREQUENCE'] - dftr['REVENUE__*__FREQUENCE_RECH']\n",
    "    dfts['REVENUE__*__FREQUENCE__-__REVENUE__*__FREQUENCE_RECH'] = dfts['REVENUE__*__FREQUENCE'] - dfts['REVENUE__*__FREQUENCE_RECH']\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('FREQ_TOP_PACK__FILLED')\n",
    "    dftr['FREQ_TOP_PACK__FILLED'] = dftr['FREQ_TOP_PACK'].fillna(0)\n",
    "    dfts['FREQ_TOP_PACK__FILLED'] = dfts['FREQ_TOP_PACK'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('MONTANT__FILLED')\n",
    "    dftr['MONTANT__FILLED'] = dftr['MONTANT'].fillna(np.median(dftr[~dftr['MONTANT'].isna()]['MONTANT']))\n",
    "    dfts['MONTANT__FILLED'] = dfts['MONTANT'].fillna(np.median(dftr[~dftr['MONTANT'].isna()]['MONTANT']))\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG__MONTANT')\n",
    "    dftr['LOG__MONTANT'] = dftr['MONTANT__FILLED'].apply(np.log)\n",
    "    dfts['LOG__MONTANT'] = dfts['MONTANT__FILLED'].apply(np.log)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('DATA_VOLUME__FILLED')\n",
    "    dftr['DATA_VOLUME__FILLED'] = dftr['DATA_VOLUME'].fillna(np.median(dftr[~dftr['DATA_VOLUME'].isna()]['DATA_VOLUME']))\n",
    "    dfts['DATA_VOLUME__FILLED'] = dfts['DATA_VOLUME'].fillna(np.median(dftr[~dftr['DATA_VOLUME'].isna()]['DATA_VOLUME']))\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG1P__DATA_VOLUME')\n",
    "    dftr['LOG1P__DATA_VOLUME'] = dftr['DATA_VOLUME__FILLED'].apply(np.log1p)\n",
    "    dfts['LOG1P__DATA_VOLUME'] = dfts['DATA_VOLUME__FILLED'].apply(np.log1p)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('ON_NET__FILLED')\n",
    "    dftr['ON_NET__FILLED'] = dftr['ON_NET'].fillna(0)\n",
    "    dfts['ON_NET__FILLED'] = dfts['ON_NET'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ORANGE__FILLED')\n",
    "    dftr['ORANGE__FILLED'] = dftr['ORANGE'].fillna(0)\n",
    "    dfts['ORANGE__FILLED'] = dfts['ORANGE'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('TIGO__FILLED')\n",
    "    dftr['TIGO__FILLED'] = dftr['TIGO'].fillna(0)\n",
    "    dfts['TIGO__FILLED'] = dfts['TIGO'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ZONE1__FILLED')\n",
    "    dftr['ZONE1__FILLED'] = dftr['ZONE1'].fillna(0)\n",
    "    dfts['ZONE1__FILLED'] = dfts['ZONE1'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ZONE2__FILLED')\n",
    "    dftr['ZONE2__FILLED'] = dftr['ZONE2'].fillna(0)\n",
    "    dfts['ZONE2__FILLED'] = dfts['ZONE2'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ON_NET__-__ZONES')\n",
    "    dftr['ON_NET__-__ZONES'] = dftr['ON_NET__FILLED'] - (dftr['ZONE2__FILLED'] + dftr['ZONE1__FILLED'])\n",
    "    dfts['ON_NET__-__ZONES'] = dfts['ON_NET__FILLED'] - (dfts['ZONE2__FILLED'] + dfts['ZONE1__FILLED'])\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ON_NET__-__OT')\n",
    "    dftr['ON_NET__-__OT'] = dftr['ON_NET__FILLED'] - (dftr['ORANGE__FILLED'] + dftr['TIGO__FILLED'])\n",
    "    dfts['ON_NET__-__OT'] = dfts['ON_NET__FILLED'] - (dfts['ORANGE__FILLED'] + dfts['TIGO__FILLED'])\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG__DATA_VOLUME__+1__/__ON_NET__+1')\n",
    "    dftr['LOG__DATA_VOLUME__+1__/__ON_NET__+1'] = ((dftr['DATA_VOLUME__FILLED'] + 1) / (dftr['ON_NET__FILLED'] + 1)).apply(np.log)\n",
    "    dfts['LOG__DATA_VOLUME__+1__/__ON_NET__+1'] = ((dfts['DATA_VOLUME__FILLED'] + 1) / (dfts['ON_NET__FILLED'] + 1)).apply(np.log)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('ARPU_SEGMENT__FILLED')\n",
    "    dftr['ARPU_SEGMENT__FILLED'] = dftr['ARPU_SEGMENT'].fillna(0)\n",
    "    dfts['ARPU_SEGMENT__FILLED'] = dfts['ARPU_SEGMENT'].fillna(0)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ARPU_SEGMENT__/__REVENUE')\n",
    "    dftr['ARPU_SEGMENT__/__REVENUE'] = dftr['ARPU_SEGMENT__FILLED'] / dftr['REVENUE__FILLED']\n",
    "    dfts['ARPU_SEGMENT__/__REVENUE'] = dfts['ARPU_SEGMENT__FILLED'] / dfts['REVENUE__FILLED']\n",
    "    dftr.loc[(dftr['ARPU_SEGMENT__FILLED'] == 0) & (dftr['REVENUE__FILLED'] == 0), 'ARPU_SEGMENT__/__REVENUE'] = 1\n",
    "    dfts.loc[(dfts['ARPU_SEGMENT__FILLED'] == 0) & (dfts['REVENUE__FILLED'] == 0), 'ARPU_SEGMENT__/__REVENUE'] = 1\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG1P__REVENUE__-__ARPU_SEGMENT')\n",
    "    dftr['LOG1P__REVENUE__-__ARPU_SEGMENT'] = (dftr['REVENUE__FILLED'] - dftr['ARPU_SEGMENT__FILLED']).apply(np.log1p)\n",
    "    dfts['LOG1P__REVENUE__-__ARPU_SEGMENT'] = (dfts['REVENUE__FILLED'] - dfts['ARPU_SEGMENT__FILLED']).apply(np.log1p)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    categorical_features = ['TENURE', 'REGION']\n",
    "    continuous_features = ['REVENUE', 'MONTANT', 'ARPU_SEGMENT', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO', 'ZONE1', 'ZONE2']\n",
    "    \n",
    "    for conf in continuous_features:\n",
    "        for catf in categorical_features:\n",
    "            dftr, dfts, new_feature_names = gb_features(dftr, dfts, catf, conf)\n",
    "            new_features += new_feature_names\n",
    "            print(new_features[-2:], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    dftr['REGION__FILLED'] = dftr['REGION'].fillna('unk')\n",
    "    dfts['REGION__FILLED'] = dfts['REGION'].fillna('unk')\n",
    "    dftr, dfts, fname = rank_target_encoding(dftr, dfts, 'REGION__FILLED')\n",
    "    new_features.append(fname)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    dftr['TENURE__FILLED'] = dftr['TENURE'].fillna('unk')\n",
    "    dfts['TENURE__FILLED'] = dfts['TENURE'].fillna('unk')\n",
    "    dftr, dfts, fname = rank_target_encoding(dftr, dfts, 'TENURE__FILLED')\n",
    "    new_features.append(fname)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    dftr['TOP_PACK__FILLED'] = dftr['TOP_PACK'].fillna('unk')\n",
    "    dfts['TOP_PACK__FILLED'] = dfts['TOP_PACK'].fillna('unk')\n",
    "    dftr, dfts, fname = rank_target_encoding(dftr, dfts, 'TOP_PACK__FILLED')\n",
    "    dfts.loc[dfts[fname].isna(), fname] = dftr.loc[dftr['TOP_PACK'].isna(), fname].values[0]\n",
    "    new_features.append(fname)\n",
    "    print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('user_id_int')\n",
    "    sha1_hashes = defaultdict(lambda: -1, {sha1(str(i).encode('utf-8')).hexdigest(): i for i in range(10000000)})\n",
    "    dftr['user_id_int'] = dftr['user_id'].map(sha1_hashes)\n",
    "    dfts['user_id_int'] = dfts['user_id'].map(sha1_hashes)\n",
    "    print(new_features[-1])\n",
    "    \n",
    "    return dftr, dfts, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr = pd.read_csv('../data_orig/Train.csv')\n",
    "dfts = pd.read_csv('../data_orig/Test.csv')\n",
    "dftr, dfts, new_features = add_features(dftr, dfts)\n",
    "predictors = ['REGULARITY'] + new_features\n",
    "predictors = list(sorted(list(set(dftr[predictors].columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(data, shuffle, predictors, target, seed=None):\n",
    "    y = data[target].copy()\n",
    "    if shuffle:\n",
    "        y = data[target].copy().sample(frac=1.0)\n",
    "    \n",
    "    dtrain = lgb.Dataset(data[predictors], y, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'subsample': 0.623,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'seed': seed,\n",
    "        'bagging_freq': 1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df[\"feature\"] = predictors\n",
    "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[predictors]))\n",
    "    \n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_df = pd.DataFrame()\n",
    "start = time.time()\n",
    "nb_runs = 80\n",
    "dsp = ''\n",
    "\n",
    "for i in range(nb_runs):\n",
    "    imp_df = get_feature_importances(data=dftr, shuffle=True, predictors=predictors, target='CHURN')\n",
    "    imp_df['run'] = i + 1 \n",
    "    \n",
    "    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    for l in range(len(dsp)):\n",
    "        print('\\b', end='', flush=True)\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = f'Done with {i + 1} of {nb_runs} (Spent {spent:5.1f} min)'\n",
    "    print(dsp, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3333)\n",
    "actual_imp_df = get_feature_importances(data=dftr, shuffle=False, predictors=predictors, target='CHURN', seed=3333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    # Plot Split importances\n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n",
    "    # Plot Gain importances\n",
    "    ax = plt.subplot(gs[0, 1])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ff in predictors:\n",
    "    display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_=ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    gain_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    split_score = 100 * (f_null_imps < np.percentile(f_act_imps, 25)).sum() / f_null_imps.size\n",
    "    correlation_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "corr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\n",
    "fig.subplots_adjust(top=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors2 = corr_scores_df[corr_scores_df['split_score'] > 0]['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = lgb.LGBMClassifier(boosting_type='dart', class_weight='balanced', n_estimators=1000, num_leaves=64, learning_rate=0.03, reg_lambda=0.00001, reg_alpha=0.00001, subsample=0.8, colsample_bytree=0.8, random_state=3333, verbose=1)\n",
    "mdl.fit(dftr[predictors2], dftr['CHURN'])\n",
    "preds_test = mdl.predict_proba(dfts[predictors2])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'user_id': dfts['user_id'], 'CHURN': 1 - preds_test})  # hide score\n",
    "sub.to_csv('../submissions/new_features__first.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-amber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-interval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-declaration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-subject",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
