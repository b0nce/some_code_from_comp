{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from hashlib import sha1\n",
    "\n",
    "from model_saver import save_params_scores\n",
    "\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "pd.options.display.max_columns = 120\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gb_features(df, cat, num):\n",
    "    feature_name_diff = f'GB_DIFF_FEATURE__{cat}__{num}'\n",
    "    feature_name = f'GB_FEATURE__{cat}__{num}'\n",
    "    \n",
    "    mapping_mean = dict(df.groupby(cat, dropna=False)[num].agg('mean'))\n",
    "    \n",
    "    df[feature_name] = df[cat].map(mapping_mean)\n",
    "    df[feature_name_diff] = df[num] - df[feature_name]\n",
    "\n",
    "    return df, [feature_name_diff, feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pack_mapping = {\n",
    "    '1000=Unlimited7Day': [7, float('nan'), 1000, -1, float('nan'), 1],\n",
    "    '1500=Unlimited7Day': [7, float('nan'), 1500, -1, float('nan'), 1],\n",
    "    '150=unlimited pilot auto': [float('nan'), float('nan'), 150, -1, float('nan'), 1],\n",
    "    '200=Unlimited1Day': [1, float('nan'), 200, -1, float('nan'), 1],\n",
    "    '200=unlimited pilot auto': [float('nan'), float('nan'), 200, -1, float('nan'), 1],\n",
    "    '200F=10mnOnNetValid1H': [0.04167, float('nan'), 200, float('nan'), float('nan'), 2],\n",
    "    '301765007': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    '305155009': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    '500=Unlimited3Day': [3, float('nan'), 500, -1, float('nan'), 1],\n",
    "    'APANews_monthly': [float('nan'), 30, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'APANews_weekly': [float('nan'), 7, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'All-net 1000=5000;5d': [5, float('nan'), 1000, 5000, float('nan'), 3],\n",
    "    'All-net 1000F=(3000F On+3000F Off);5d': [5, float('nan'), 1000, 6000, float('nan'), 3],\n",
    "    'All-net 300=600;2d': [2, float('nan'), 300, 600, float('nan'), 3],\n",
    "    'All-net 5000= 20000off+20000on;30d': [30, float('nan'), 5000, 40000, float('nan'), 3],\n",
    "    'All-net 500= 4000off+4000on;24H': [1, float('nan'), 500, 8000, float('nan'), 3],\n",
    "    'All-net 500F =2000F_AllNet_Unlimited': [float('nan'), float('nan'), 500, -1, float('nan'), 3],\n",
    "    'All-net 500F=1250F_AllNet_1250_Onnet;48h': [2, float('nan'), 500, 2500, float('nan'), 3],\n",
    "    'All-net 500F=2000F;5d': [5, float('nan'), 500, 2000, float('nan'), 3],\n",
    "    'All-net 500F=4000F ; 5d': [5, float('nan'), 500, 4000, float('nan'), 3],\n",
    "    'All-net 600F= 3000F ;5d': [5, float('nan'), 600, 3000, float('nan'), 3],\n",
    "    'CVM_100F_unlimited': [float('nan'), float('nan'), 100, -1, float('nan'), 3],\n",
    "    'CVM_100f=200 MB': [float('nan'), float('nan'), 100, float('nan'), 200, 5],\n",
    "    'CVM_100f=500 onNet': [float('nan'), float('nan'), 100, 500, float('nan'), 3],\n",
    "    'CVM_150F_unlimited': [float('nan'), float('nan'), 150, -1, float('nan'), 3],\n",
    "    'CVM_200f=400MB': [float('nan'), float('nan'), 200, float('nan'), 400, 5],\n",
    "    'CVM_500f=2GB': [float('nan'), float('nan'), 500, float('nan'), 2000, 5],\n",
    "    'CVM_On-net 1300f=12500': [float('nan'), float('nan'), 1300, 12500, float('nan'), 3],\n",
    "    'CVM_On-net 400f=2200F': [float('nan'), float('nan'), 400, 2200, float('nan'), 3],\n",
    "    'CVM_on-net bundle 500=5000': [float('nan'), float('nan'), 500, 5000, float('nan'), 3],\n",
    "    'Data: 100 F=40MB,24H': [1, float('nan'), 100, float('nan'), 40, 5],\n",
    "    'Data: 200 F=100MB,24H': [1, float('nan'), 200, float('nan'), 100, 5],\n",
    "    'Data: 200F=1GB,24H': [1, float('nan'), 200, float('nan'), 1000, 5],\n",
    "    'Data: 490F=Night,00H-08H': [0.33333, float('nan'), 490, float('nan'), -1, 5],\n",
    "    'Data:1000F=2GB,30d': [30, float('nan'), 1000, float('nan'), 2000, 5],\n",
    "    'Data:1000F=5GB,7d': [7, float('nan'), 1000, float('nan'), 5000, 5],\n",
    "    'Data:1000F=700MB,7d': [7, float('nan'), 1000, float('nan'), 700, 5],\n",
    "    'Data:1500F=3GB,30D': [30, float('nan'), 1500, float('nan'), 3000, 5],\n",
    "    'Data:1500F=SPPackage1,30d': [30, float('nan'), 1500, float('nan'), float('nan'), 4],\n",
    "    'Data:150F=SPPackage1,24H': [1, float('nan'), 150, float('nan'), float('nan'), 4],\n",
    "    'Data:200F=Unlimited,24H': [1, float('nan'), 200, -1, -1, 4],\n",
    "    'Data:3000F=10GB,30d': [30, float('nan'), 3000, float('nan'), 10000, 5],\n",
    "    'Data:300F=100MB,2d': [2, float('nan'), 300, float('nan'), 100, 5],\n",
    "    'Data:30Go_V 30_Days': [30, float('nan'), float('nan'), float('nan'), 30000, 5],\n",
    "    'Data:490F=1GB,7d': [7, float('nan'), 490, float('nan'), 1000, 5],\n",
    "    'Data:500F=2GB,24H': [1, float('nan'), 500, float('nan'), 2000, 5],\n",
    "    'Data:50F=30MB_24H': [1, float('nan'), 50, float('nan'), 30, 5],\n",
    "    'Data:700F=1.5GB,7d': [7, float('nan'), 700, float('nan'), 1500, 5],\n",
    "    'Data:700F=SPPackage1,7d': [7, float('nan'), 700, float('nan'), float('nan'), 4],\n",
    "    'Data:DailyCycle_Pilot_1.5GB': [float('nan'), 1, float('nan'), float('nan'), 1500, 5],\n",
    "    'Data:New-GPRS_PKG_1500F': [float('nan'), float('nan'), float('nan'), 1500, float('nan'), 4],\n",
    "    'Data:OneTime_Pilot_1.5GB': [float('nan'), float('nan'), float('nan'), float('nan'), 1500, 5],\n",
    "    'DataPack_Incoming': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'Data_EVC_2Go24H': [1, float('nan'), float('nan'), float('nan'), 2000, 5],\n",
    "    'Data_Mifi_10Go': [float('nan'), float('nan'), float('nan'), float('nan'), 10000, 5],\n",
    "    'Data_Mifi_10Go_Monthly': [float('nan'), 30, float('nan'), float('nan'), 10000, 5],\n",
    "    'Data_Mifi_20Go': [float('nan'), float('nan'), float('nan'), float('nan'), 20000, 5],\n",
    "    'ESN_POSTPAID_CLASSIC_RENT': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'EVC_1000=6000 F': [float('nan'), float('nan'), 1000, 6000, float('nan'), 3],\n",
    "    'EVC_100Mo': [float('nan'), float('nan'), float('nan'), float('nan'), 100, 5],\n",
    "    'EVC_1Go': [float('nan'), float('nan'), float('nan'), float('nan'), 1000, 5],\n",
    "    'EVC_4900=12000F': [float('nan'), float('nan'), 4900, 12000, float('nan'), 3],\n",
    "    'EVC_500=2000F': [float('nan'), float('nan'), 500, 2000, float('nan'), 3],\n",
    "    'EVC_700Mo': [float('nan'), float('nan'), float('nan'), float('nan'), 700, 5],\n",
    "    'EVC_JOKKO30': [float('nan'), 30, float('nan'), float('nan'), float('nan'), 1],\n",
    "    'EVC_Jokko_Weekly': [float('nan'), 7, float('nan'), float('nan'), float('nan'), 1],\n",
    "    'EVC_MEGA10000F': [float('nan'), float('nan'), float('nan'), 10000, float('nan'), 3],\n",
    "    'EVC_PACK_2.2Go': [float('nan'), float('nan'), float('nan'), float('nan'), 2200, 5],\n",
    "    'FIFA_TS_daily': [float('nan'), 1, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'FIFA_TS_monthly': [float('nan'), 30, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'FIFA_TS_weekly': [float('nan'), 7, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'FNF2 ( JAPPANTE)': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'FNF_Youth_ESN': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'Facebook_MIX_2D': [2, float('nan'), float('nan'), float('nan'), float('nan'), 4],\n",
    "    'GPRS_3000Equal10GPORTAL': [float('nan'), float('nan'), 3000, float('nan'), 10000, 5],\n",
    "    'GPRS_5Go_7D_PORTAL': [7, float('nan'), float('nan'), float('nan'), 5000, 5],\n",
    "    'GPRS_BKG_1000F MIFI': [float('nan'), float('nan'), float('nan'), 1000, float('nan'), 3],\n",
    "    'GPRS_PKG_5GO_ILLIMITE': [float('nan'), float('nan'), float('nan'), float('nan'), 5000, 5],\n",
    "    'Go-NetPro-4 Go': [float('nan'), float('nan'), float('nan'), float('nan'), 4000, 5],\n",
    "    'IVR Echat_Daily_50F': [float('nan'), 1, 50, float('nan'), float('nan'), 4],\n",
    "    'IVR Echat_Monthly_500F': [float('nan'), 30, 500, float('nan'), float('nan'), 4],\n",
    "    'IVR Echat_Weekly_200F': [float('nan'), 7, 200, float('nan'), float('nan'), 4],\n",
    "    'Incoming_Bonus_woma': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'Internat: 1000F_Zone_1;24H\t\t': [1, float('nan'), 1000, float('nan'), float('nan'), 2],\n",
    "    'Internat: 1000F_Zone_3;24h\t\t': [1, float('nan'), 1000, float('nan'), float('nan'), 2],\n",
    "    'Internat: 2000F_Zone_2;24H\t\t': [1, float('nan'), 2000, float('nan'), float('nan'), 2],\n",
    "    'Jokko_Daily': [float('nan'), 1, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'Jokko_Monthly': [float('nan'), 30, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'Jokko_Weekly': [float('nan'), 7, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'Jokko_promo': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 4],\n",
    "    'MIXT: 200mnoff net _unl on net _5Go;30d': [30, float('nan'), float('nan'), -1, 5000, 2],\n",
    "    'MIXT: 390F=04HOn-net_400SMS_400 Mo;4h\t': [0.16667, float('nan'), 390, float('nan'), 400, 2],\n",
    "    'MIXT: 4900F= 10H on net_1,5Go ;30d': [30, float('nan'), 4900, float('nan'), 5000, 2],\n",
    "    'MIXT: 5000F=80Konnet_20Koffnet_250Mo;30d\t\t': [30, float('nan'), 5000, 100000, 250, 3], \n",
    "    'MIXT: 500F=75(SMS, ONNET, Mo)_1000FAllNet;24h\t\t': [1, float('nan'), 500, 1000, 75, 3],\n",
    "    'MIXT: 590F=02H_On-net_200SMS_200 Mo;24h\t\t': [1, float('nan'), 590, float('nan'), 200, 2],\n",
    "    'MIXT:10000F=10hAllnet_3Go_1h_Zone3;30d\t\t': [30, float('nan'), 10000, float('nan'), 3000, 2],\n",
    "    'MIXT:1000F=4250 Off net _ 4250F On net _100Mo; 5d': [5, float('nan'), 1000, 8500, 100, 3],\n",
    "    'MIXT:500F= 2500F on net _2500F off net;2d': [2, float('nan'), 500, 2500, float('nan'), 3],\n",
    "    'MROMO_TIMWES_OneDAY': [1, float('nan'), float('nan'), float('nan'), float('nan'), 4],\n",
    "    'MROMO_TIMWES_RENEW': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 4],\n",
    "    'MegaChrono_3000F=12500F TOUS RESEAUX': [float('nan'), float('nan'), 3000, 12500, float('nan'), 3],\n",
    "    'Mixt 250F=Unlimited_call24H': [1, float('nan'), 250, -1, float('nan'), 2],\n",
    "    'Mixt : 500F=2500Fonnet_2500Foffnet ;5d': [5, float('nan'), 500, 5000, float('nan'), 3],\n",
    "    'NEW_CLIR_PERMANENT_LIBERTE_MOBILE': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 4],\n",
    "    'NEW_CLIR_TEMPALLOWED_LIBERTE_MOBILE': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 4],\n",
    "    'NEW_CLIR_TEMPRESTRICTED_LIBERTE_MOBILE': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 4],\n",
    "    'New_YAKALMA_4_ALL': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'On net 200F= 3000F_10Mo ;24H': [1, float('nan'), 200, 3000, 10, 3],\n",
    "    'On net 200F=Unlimited _call24H': [1, float('nan'), 200, -1, float('nan'), 2],\n",
    "    'On-net 1000F=10MilF;10d': [10, float('nan'), 1000, -1, float('nan'), 3],\n",
    "    'On-net 2000f_One_Month_100H; 30d': [30, float('nan'), 2000, float('nan'), float('nan'), 2],\n",
    "    'On-net 200F=60mn;1d': [1, float('nan'), 200, float('nan'), float('nan'), 2],\n",
    "    'On-net 300F=1800F;3d': [3, float('nan'), 300, 1800, float('nan'), 3],\n",
    "    'On-net 500=4000,10d': [10, float('nan'), 500, 4000, float('nan'), 3],\n",
    "    'On-net 500F_FNF;3d': [3, float('nan'), 500, float('nan'), float('nan'), 1],\n",
    "    'Package3_Monthly': [float('nan'), 30, float('nan'), float('nan'), float('nan'), 1],\n",
    "    'Pilot_Youth1_290': [float('nan'), float('nan'), 290, float('nan'), float('nan'), 4],\n",
    "    'Pilot_Youth4_490': [float('nan'), float('nan'), 490, float('nan'), float('nan'), 4],\n",
    "    'Postpaid FORFAIT 10H Package': [0.41667, float('nan'), float('nan'), float('nan'), float('nan'), 2],\n",
    "    'SMS Max': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'SUPERMAGIK_1000': [float('nan'), float('nan'), 1000, float('nan'), float('nan'), 1],\n",
    "    'SUPERMAGIK_5000': [float('nan'), float('nan'), 5000, float('nan'), float('nan'), 1],\n",
    "    'Staff_CPE_Rent': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'TelmunCRBT_daily': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'Twter_U2opia_Daily': [float('nan'), 1, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'Twter_U2opia_Monthly': [float('nan'), 30, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'Twter_U2opia_Weekly': [float('nan'), 7, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'VAS(IVR_Radio_Daily)': [float('nan'), 1, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'VAS(IVR_Radio_Monthly)': [float('nan'), 30, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'VAS(IVR_Radio_Weekly)': [float('nan'), 7, float('nan'), float('nan'), float('nan'), 4],\n",
    "    'WIFI_ Family _10MBPS': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 5],\n",
    "    'WIFI_ Family _4MBPS': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 5],\n",
    "    'WIFI_Family_2MBPS': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 5],\n",
    "    'YMGX 100=1 hour FNF, 24H/1 month': [1, float('nan'), 100, float('nan'), float('nan'), 2],\n",
    "    'YMGX on-net 100=700F, 24H': [1, float('nan'), 100, 700, float('nan'), 3],\n",
    "    'Yewouleen_PKG': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    float('nan'): [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'pack_chinguitel_24h': [1, float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'pilot_offer4': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'pilot_offer5': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'pilot_offer6': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1],\n",
    "    'pilot_offer7': [float('nan'), float('nan'), float('nan'), float('nan'), float('nan'), 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df, verbose=False):\n",
    "    new_features = []\n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('NANS_NUM')\n",
    "    df['NANS_NUM'] = df.isna().sum(1)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('REVENUE__ISNA')\n",
    "    df['REVENUE__ISNA'] = df['REVENUE'].isna()\n",
    "    df['REVENUE'] = df['REVENUE'].fillna(500)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('FREQUENCE__ISNA')\n",
    "    df['FREQUENCE__ISNA'] = df['FREQUENCE'].isna()\n",
    "    df['FREQUENCE'] = df['FREQUENCE'].fillna(1)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('FREQUENCE_RECH__ISNA')\n",
    "    df['FREQUENCE_RECH__ISNA'] = df['FREQUENCE_RECH'].isna()\n",
    "    df['FREQUENCE_RECH'] = df['FREQUENCE_RECH'].fillna(1)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('REVENUE__*__FREQUENCE_RECH')\n",
    "    df['REVENUE__*__FREQUENCE_RECH'] = df['REVENUE'] * df['FREQUENCE_RECH']\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('REVENUE__*__FREQUENCE')\n",
    "    df['REVENUE__*__FREQUENCE'] = df['REVENUE'] * df['FREQUENCE']\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('REVENUE__*__FREQUENCE__-__REVENUE__*__FREQUENCE_RECH')\n",
    "    df['REVENUE__*__FREQUENCE__-__REVENUE__*__FREQUENCE_RECH'] = df['REVENUE__*__FREQUENCE'] - df['REVENUE__*__FREQUENCE_RECH']\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('FREQ_TOP_PACK__ISNA')\n",
    "    df['FREQ_TOP_PACK__ISNA'] = df['FREQ_TOP_PACK'].isna()\n",
    "    df['FREQ_TOP_PACK'] = df['FREQ_TOP_PACK'].fillna(1)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('MONTANT__ISNA')\n",
    "    df['MONTANT__ISNA'] = df['MONTANT'].isna()\n",
    "    df['MONTANT'] = df['MONTANT'].fillna(df['MONTANT'].mode()[0])\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG__MONTANT')\n",
    "    df['LOG__MONTANT'] = df['MONTANT'].apply(np.log)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('DATA_VOLUME__ISNA')\n",
    "    df['DATA_VOLUME__ISNA'] = df['DATA_VOLUME'].isna()\n",
    "    df['DATA_VOLUME'] = df['DATA_VOLUME'].fillna(np.median(df[~df['DATA_VOLUME'].isna()]['DATA_VOLUME']))\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG1P__DATA_VOLUME')\n",
    "    df['LOG1P__DATA_VOLUME'] = df['DATA_VOLUME'].apply(np.log1p)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('ON_NET__ISNA')\n",
    "    df['ON_NET__ISNA'] = df['ON_NET'].isna()\n",
    "    df['ON_NET'] = df['ON_NET'].fillna(0)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ORANGE__ISNA')\n",
    "    df['ORANGE__ISNA'] = df['ORANGE'].isna()\n",
    "    df['ORANGE'] = df['ORANGE'].fillna(1)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('TIGO__ISNA')\n",
    "    df['TIGO__ISNA'] = df['TIGO'].isna()\n",
    "    df['TIGO'] = df['TIGO'].fillna(1)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ZONE1__ISNA')\n",
    "    df['ZONE1__ISNA'] = df['ZONE1'].isna()\n",
    "    df['ZONE1'] = df['ZONE1'].fillna(0)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ZONE2__ISNA')\n",
    "    df['ZONE2__ISNA'] = df['ZONE2'].isna()\n",
    "    df['ZONE2'] = df['ZONE2'].fillna(0)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ON_NET__-__ZONES')\n",
    "    df['ON_NET__-__ZONES'] = df['ON_NET'] - (df['ZONE2'] + df['ZONE1'])\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ON_NET__-__OT')\n",
    "    df['ON_NET__-__OT'] = df['ON_NET'] - (df['ORANGE'] + df['TIGO'])\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG__DATA_VOLUME__+1__/__ON_NET__+1')\n",
    "    df['LOG__DATA_VOLUME__+1__/__ON_NET__+1'] = ((df['DATA_VOLUME'] + 1) / (df['ON_NET'] + 1)).apply(np.log)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    new_features.append('ARPU_SEGMENT__ISNA')\n",
    "    df['ARPU_SEGMENT__ISNA'] = df['ARPU_SEGMENT'].isna()\n",
    "    df['ARPU_SEGMENT'] = df['ARPU_SEGMENT'].fillna(0)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('ARPU_SEGMENT__/__REVENUE')\n",
    "    df['ARPU_SEGMENT__/__REVENUE'] = df['ARPU_SEGMENT'] / df['REVENUE']\n",
    "    df.loc[(df['ARPU_SEGMENT'] == 0) & (df['REVENUE'] == 0), 'ARPU_SEGMENT__/__REVENUE'] = 1\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    new_features.append('LOG1P__REVENUE__-__ARPU_SEGMENT')\n",
    "    df['LOG1P__REVENUE__-__ARPU_SEGMENT'] = (df['REVENUE'] - df['ARPU_SEGMENT']).apply(np.log1p)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "        \n",
    "    # # # # # \n",
    "    POP = {\n",
    "        'FATICK': 24_243, \n",
    "        np.nan: -1, \n",
    "        'DAKAR': 1_056_000, \n",
    "        'LOUGA': 104_000, \n",
    "        'TAMBACOUNDA': 872_156 , \n",
    "        'KAOLACK': 410_577, \n",
    "        'THIES': 618_436,\n",
    "        'SAINT-LOUIS': 277_245, \n",
    "        'KOLDA': 62_258, \n",
    "        'KAFFRINE': 39_357, \n",
    "        'DIOURBEL': 279_667, \n",
    "        'ZIGUINCHOR': 337_295,\n",
    "        'MATAM': 17_324, \n",
    "        'SEDHIOU': 24_213, \n",
    "        'KEDOUGOU': 18_860\n",
    "    }\n",
    "    # International wealth index\n",
    "    IWI = {\n",
    "        'FATICK': 51.3, \n",
    "        np.nan: 35, \n",
    "        'DAKAR': 75.3, \n",
    "        'LOUGA': 55.3, \n",
    "        'TAMBACOUNDA': 47.5 , \n",
    "        'KAOLACK': 53.5, \n",
    "        'THIES': 68.6,\n",
    "        'SAINT-LOUIS': 52.3, \n",
    "        'KOLDA': 39.6, \n",
    "        'KAFFRINE': 45, \n",
    "        'DIOURBEL': 55.8, \n",
    "        'ZIGUINCHOR': 55.3,\n",
    "        'MATAM': 46, \n",
    "        'SEDHIOU': 45.5, \n",
    "        'KEDOUGOU': 45    \n",
    "    }\n",
    "    new_features.append('IWI')\n",
    "    df['IWI'] = df['REGION'].map(IWI).fillna(35)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    new_features.append('POP')\n",
    "    df['POP'] = df['REGION'].map(POP).fillna(-1)\n",
    "    if verbose:\n",
    "        print(new_features[-1], end=', ')\n",
    "    \n",
    "    \n",
    "    # # # # #\n",
    "    categorical_features = ['TENURE', 'REGION']\n",
    "    continuous_features = ['REVENUE', 'MONTANT', 'ARPU_SEGMENT', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO', 'ZONE1', 'ZONE2']\n",
    "    \n",
    "    for conf in continuous_features:\n",
    "        for catf in categorical_features:\n",
    "            df, new_feature_names = gb_features(df, catf, conf)\n",
    "            new_features += new_feature_names\n",
    "            if verbose:\n",
    "                print(new_features[-3:], end=', ')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['TENURE'] = le.fit_transform(df['TENURE'])\n",
    "    \n",
    "    # # # # #\n",
    "    new_features += ['TOP_PACK_l', 'TOP_PACK_r', 'TOP_PACK_p', 'TOP_PACK_m', 'TOP_PACK_i', 'TOP_PACK_t']\n",
    "    df = pd.concat(\n",
    "        (\n",
    "            df,\n",
    "            pd.DataFrame(dict(zip(\n",
    "                ['TOP_PACK_l', 'TOP_PACK_r', 'TOP_PACK_p', 'TOP_PACK_m', 'TOP_PACK_i', 'TOP_PACK_t'], \n",
    "                np.stack(df[\"TOP_PACK\"].map(top_pack_mapping).values).T\n",
    "            )))\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    if verbose:\n",
    "        print(['TOP_PACK_l', 'TOP_PACK_r', 'TOP_PACK_p', 'TOP_PACK_m', 'TOP_PACK_i', 'TOP_PACK_t'])\n",
    "    \n",
    "    return df, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sha1_hashes = defaultdict(lambda: -1, {sha1(str(i).encode('utf-8')).hexdigest(): i for i in range(10000000)})\n",
    "def add_int_uid(df):\n",
    "    df['user_id_int'] = df['user_id'].map(sha1_hashes)\n",
    "    churn = df['CHURN'].values[np.argsort(df['user_id_int'].values)]\n",
    "    churn[np.isnan(churn)] = 0\n",
    "    df['user_id_int'] = savgol_filter(churn, 24001, 2)[np.argsort(np.argsort(df['user_id_int'].values))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_isna_features(df):\n",
    "    return (\n",
    "        df['REVENUE'].isna(), \n",
    "        df['FREQUENCE'].isna(), \n",
    "        df['FREQUENCE_RECH'].isna(), \n",
    "        df['FREQ_TOP_PACK'].isna(), \n",
    "        df['MONTANT'].isna(), \n",
    "        df['DATA_VOLUME'].isna(), \n",
    "        df['ON_NET'].isna(), \n",
    "        df['ORANGE'].isna(), \n",
    "        df['TIGO'].isna(), \n",
    "        df['ZONE1'].isna(), \n",
    "        df['ZONE2'].isna(), \n",
    "        df['ARPU_SEGMENT'].isna(), \n",
    "        df['TOP_PACK'].isna()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr = pd.read_csv('../data_orig/Train.csv')\n",
    "dfts = pd.read_csv('../data_orig/Test.csv')\n",
    "df = pd.concat([dftr, dfts]).reset_index(drop=True)\n",
    "df = add_int_uid(df)\n",
    "df, new_features = add_features(df)\n",
    "df.drop(columns=['TOP_PACK', 'MRG', 'REGION'], inplace=True)\n",
    "dftr = df.iloc[:len(dftr), :]\n",
    "dfts = df.iloc[len(dftr):, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [c for c in dftr.columns if not c in ['user_id', 'CHURN']]\n",
    "target = ['CHURN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns[df.nunique() < 50]:\n",
    "    if c == 'CHURN':\n",
    "        continue\n",
    "    for u in dftr[c].unique():\n",
    "        if not (dfts[c] == u).any():\n",
    "            print(c, u, len(df), end=' -> ')\n",
    "            dftr = dftr[~(dftr[c] == u)]\n",
    "            print(len(dftr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import boostaroota as br\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dftr[predictors].astype(float).values, dftr[target].values\n",
    "mdl = LGBMClassifier(class_weight='balanced', n_estimators=500, learning_rate=0.075, silent=False)\n",
    "mdl.fit(X, y)\n",
    "\n",
    "remove = [k for k, v in dict(zip(predictors, mdl.feature_importances_)).items() if v < 10]\n",
    "predictors2 = [p for p in predictors if not p in remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "brm = br.BoostARoota(clf=LGBMClassifier(class_weight='balanced', n_estimators=500, learning_rate=0.075, silent=False))\n",
    "dftr_new = brm.fit_transform(dftr[predictors2], y)\n",
    "brm = br.BoostARoota(clf=LGBMClassifier(class_weight='balanced', n_estimators=500, learning_rate=0.075, silent=False))\n",
    "dftr_new_v2 = brm.fit_transform(dftr[predictors2], y)\n",
    "brm = br.BoostARoota(clf=LGBMClassifier(class_weight='balanced', n_estimators=500, learning_rate=0.075, silent=False))\n",
    "dftr_new_v3 = brm.fit_transform(dftr[predictors2], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "less_cols = list(sorted(list(set(dftr_new.columns).intersection(set(dftr_new_v2.columns)).intersection(set(dftr_new_v3.columns)))))\n",
    "more_cols = list(sorted(list(set(dftr_new.columns.tolist() + dftr_new_v2.columns.tolist() + dftr_new_v3.columns.tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(less_cols, open('less_cols.json', 'w'))\n",
    "json.dump(more_cols, open('more_cols.json', 'w'))\n",
    "X, y = dftr[less_cols].astype(float).values, dftr[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr['user_id_int2'] = dftr['user_id'].map(sha1_hashes)\n",
    "dfts['user_id_int2'] = dfts['user_id'].map(sha1_hashes)\n",
    "less_cols2 = less_cols[:]\n",
    "less_cols2[less_cols2.index('user_id_int')] = 'user_id_int2'\n",
    "X2 = dftr[less_cols2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = cb.CatBoostClassifier(iterations=1000, task_type='GPU', l2_leaf_reg=0.03, learning_rate=0.05)\n",
    "mdl.fit(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ts = mdl.predict_proba(dfts[less_cols2])[:, 1]\n",
    "df1 = pd.read_csv('../submissions/new_features__first.csv')\n",
    "df2 = pd.read_csv('../submissions/hard_dart_weightned_local_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ts1 = (1 - df1['CHURN'])  # real score\n",
    "pred_ts2 = (1 - df2['CHURN'])  # real score\n",
    "pred_ts3 = np.copy(preds_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ultimate = pred_ts1 + pred_ts2 + pred_ts3\n",
    "pred_ultimate /= max(pred_ultimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfts['CHURN'] = pred_ultimate\n",
    "dfts[['user_id', 'CHURN']].to_csv('../submissions/3best.csv', index=False)  # best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ultimate = 0.8 * pred_ts1 + 0.2 * pred_ts2\n",
    "dfts['CHURN'] = pred_ultimate\n",
    "dfts[['user_id', 'CHURN']].to_csv('../submissions/2best_08_02.csv', index=False)  # second best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-danger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-mainstream",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
